
Vernetzen von Programmen
========================

Während sich viele der Beispiele in diesem Buch auf das Lesen von Dateien und die Suche nach Daten in diesen Dateien konzentriert haben, gibt es viele verschiedene Informationsquellen, wenn man das Internet betrachtet.

In diesem Kapitel werden wir so tun, als wären wir ein Webbrowser und würden Webseiten über das Hypertext Transfer Protocol (HTTP) abrufen. Dann werden wir die Daten der Webseite einlesen und parsen.

Hypertext Transfer Protocol - HTTP
----------------------------------

Das Netzwerkprotokoll, das das Web antreibt, ist eigentlich recht einfach und es gibt eine eingebaute Unterstützung in Python namens `socket`, die es sehr einfach macht, Netzwerkverbindungen herzustellen und Daten über diese Sockets in einem Python-Programm abzurufen.

Ein *Socket* ist ähnlich wie eine Datei, mit dem Unterschied, dass ein einzelner Socket eine bidirektionale Verbindung zwischen zwei Programmen ermöglicht. Sie können sowohl von einem Socket lesen als auch auf einen Socket schreiben. Wenn Sie etwas in eine Socket schreiben, wird es an die Anwendung am anderen Ende des Sockets gesendet. Wenn Sie aus dem Socket lesen, erhalten Sie die Daten, die die andere Anwendung gesendet hat.

Wenn Sie aber versuchen, einen Socket zu lesen, wenn das Programm am anderen Ende des Sockets keine Daten gesendet hat, sitzen Sie nur da und warten. Wenn die Programme an beiden Enden des Sockets einfach auf einige Daten warten, ohne etwas zu senden, werden sie sehr lange warten, daher ist ein wichtiger Teil von Programmen, die über das Internet kommunizieren, eine Art Protokoll zu haben.

Ein Protokoll ist ein Satz präziser Regeln, die festlegen, wer zuerst sendet, was er zu tun hat, was die Antworten auf diese Nachricht sind, wer als nächstes sendet und so weiter. In gewisser Weise führen die beiden Anwendungen an beiden Enden des Sockets einen Tanz auf und achten darauf, sich nicht gegenseitig auf die Füße zu treten.

Es gibt viele Dokumente, die diese Netzwerkprotokolle beschreiben. Das Hypertext Transfer Protocol wird in dem folgenden Dokument beschrieben:

<https://www.w3.org/Protocols/rfc2616/rfc2616.txt>

Dies ist ein langes und komplexes 176-seitiges Dokument mit vielen Details. Wer es interessant findet, kann es gerne ganz lesen. Aber wenn wir uns auf Seite 36 von RFC2616 umsehen, finden wir die Syntax für den GET-Request. Um ein Dokument von einem Webserver anzufordern, stellen wir eine Verbindung zum `www.pr4e.org`-Server auf Port 80 her und senden dann eine Zeile der Form

`GET http://data.pr4e.org/romeo.txt HTTP/1.0 `

wobei der zweite Parameter die von uns angeforderte Webseite ist. Außerdem senden wir auch eine Leerzeile. Der Webserver antwortet mit einigen Header-Informationen über das Dokument und einer Leerzeile, gefolgt von dem Dokumentinhalt.

Der einfachste Webbrowser der Welt
----------------------------------

Der vielleicht einfachste Weg zu zeigen, wie das HTTP-Protokoll funktioniert, ist, ein sehr einfaches Python-Programm zu schreiben, das eine Verbindung zu einem Webserver herstellt und den Regeln des HTTP-Protokolls folgt, um ein Dokument anzufordern und anzuzeigen, was der Server zurückschickt.

\VerbatimInput{../code3/socket1.py}

Zunächst stellt das Programm eine Verbindung zu Port 80 auf dem Server [www.py4e.com](http://www.py4e.com) her. Da unser Programm die Rolle des Webbrowsers spielt, sagt das HTTP-Protokoll, dass wir den GET-Befehl gefolgt von einer Leerzeile senden müssen. `\r\n` bedeutet ein EOL (End-of-Line), also steht `\r\n\r\n` für „nichts“ zwischen zwei EOL-Sequenzen. Das ist das Äquivalent zu einer Leerzeile.

![Eine Socket-Verbindung](height=2.0in@../images/socket)

Sobald wir diese Leerzeile gesendet haben, schreiben wir eine Schleife, die Daten in 512-Zeichen-Blöcken vom Socket empfängt und die Daten ausgibt, bis es keine weiteren Daten mehr zu lesen gibt (d. h., bis `recv()` eine leere Zeichenkette liefert).

Das Programm erzeugt die folgende Ausgabe:

~~~~
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:52:55 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Sat, 13 May 2017 11:22:22 GMT
ETag: "a7-54f6609245537"
Accept-Ranges: bytes
Content-Length: 167
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/plain

But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

Die Ausgabe beginnt mit Headern, die der Webserver sendet, um das Dokument zu beschreiben. Der Header `Content-Type` zeigt zum Beispiel an, dass es sich um ein reines Textdokument (`text/plain`) handelt.

Nachdem der Server uns die Kopfzeilen gesendet hat, fügt er eine Leerzeile ein, um das Ende der Kopfzeilen anzuzeigen, und sendet dann die eigentlichen Daten der Datei `romeo.txt`.

Dieses Beispiel zeigt, wie wir eine Low-Level-Netzwerkverbindung mit Sockets herstellen. Sockets können für die Kommunikation mit einem Webserver oder mit einem Mailserver oder vielen anderen Arten von Servern verwendet werden. Man muss nur das Dokument finden, das das Protokoll beschreibt, und dann den entsprechenden Code verwenden, um die Daten gemäß dem Protokoll zu senden und zu empfangen.

Da das von uns am häufigsten verwendete Protokoll jedoch das HTTP-Webprotokoll ist, verfügt Python über eine spezielle Bibliothek zur Unterstützung des HTTP-Protokolls für den Abruf von Dokumenten und Daten über das Web.

Eine der Voraussetzungen für die Verwendung des HTTP-Protokolls ist die Notwendigkeit, Daten als Byte-Objekte anstelle von Strings zu senden und zu empfangen. Im vorangegangenen Beispiel wandeln die Methoden `encode()` und `decode()` Strings in Byte-Objekte und wieder zurück.

Das nächste Beispiel verwendet die Notation `b''`, um anzugeben, dass eine Variable als Byte-Objekt gespeichert werden soll. `encode()` und `b''` sind gleichwertig.

~~~~
>>> b'Hello world'
b'Hello world'
>>> 'Hello world'.encode()
b'Hello world'
~~~~

Abrufen eines Bildes über HTTP
-----------------------------

\index{urllib!Bild}
\index{Bild!jpg}
\index{jpg}

Im obigen Beispiel haben wir eine reine Textdatei abgerufen, die Zeilenumbrüche in der Datei hatte, und wir haben die Daten einfach auf den Bildschirm kopiert, während das Programm lief. Wir können ein ähnliches Programm verwenden, um ein Bild über HTTP abzurufen. Anstatt die Daten bei der Ausführung des Programms auf den Bildschirm zu kopieren, sammeln wir die Daten in einer Zeichenkette, schneiden die Kopfzeilen ab und speichern die Bilddaten dann wie folgt in einer Datei:

\VerbatimInput{../code3/urljpeg.py}

Wenn das Programm läuft, erzeugt es die folgende Ausgabe:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
4240 14480
5120 19600
...
5120 214000
3200 217200
5120 222320
5120 227440
3167 230607
Header length 393
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:54:09 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Wir können sehen, dass bei dieser URL der `Content-Type`-Header anzeigt, dass der Körper des Dokuments ein Bild ist (`image/jpeg`). Sobald das Programm beendet ist, können wir die Bilddaten anzeigen, indem wir die Datei `stuff.jpg` in einem Bildbetrachter öffnen.

Während das Programm läuft, können wir sehen, dass wir nicht jedes Mal 5120 Zeichen erhalten, wenn wir die Methode `recv()` aufrufen. Wir erhalten so viele Zeichen, wie in dem Moment, in dem wir `recv()` aufrufen, vom Webserver über das Netzwerk zu uns übertragen wurden. In diesem Beispiel erhalten wir nur 3200 Zeichen, wenn wir bis zu 5120 Zeichen an Daten anfordern.

Unsere Ergebnisse können je nach der Netzwerkgeschwindigkeit unterschiedlich sein. Beachten wir auch, dass wir beim letzten Aufruf von `recv()` 3167 Bytes erhalten, was das Ende des Streams ist, und dass wir beim nächsten Aufruf von `recv()` eine Zeichenkette der Länge 0 erhalten, die uns mitteilt, dass der Server an seinem Ende des Sockets `close()` aufgerufen hat und keine weiteren Daten mehr anstehen.

\index{time}
\index{time.sleep (Funktion)}

Wir können unsere aufeinanderfolgenden `recv()`-Aufrufe verlangsamen, indem wir den Aufruf von `time.sleep()` auskommentieren. Auf diese Weise warten wir nach jedem Aufruf eine Viertelsekunde, damit der Server uns „zuvorkommen“ und weitere Daten an uns senden kann, bevor wir `recv()` erneut aufrufen. Mit der Verzögerung an Ort und Stelle wird das Programm wie folgt ausgeführt:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
5120 15360
...
5120 225280
5120 230400
207 230607
Header length 393
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 21:42:08 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Abgesehen vom ersten und letzten Aufruf von `recv()` erhalten wir jetzt jedes Mal 5120 Zeichen, wenn wir neue Daten anfordern.

Es gibt einen Puffer zwischen dem Server, der `send()`-Requests stellt, und unserer Anwendung, die `recv()`-Requests stellt. Wenn wir das Programm mit der Verzögerung laufen lassen, kann es sein, dass der Server irgendwann den Puffer im Socket füllt und gezwungen ist, eine Pause einzulegen, bis unser Programm beginnt, den Puffer zu leeren. Das Anhalten entweder der sendenden oder der empfangenden Anwendung wird *Flusskontrolle* (englisch *flow control*) genannt.

\index{flow control}\index{Flusskontrolle}

Abrufen von Webseiten mit `urllib`
----------------------------------

Während wir mit der Socket-Bibliothek manuell Daten über HTTP senden und empfangen können, gibt es einen viel einfacheren Weg, diese allgemeine Aufgabe in Python auszuführen, indem wir die `urllib`-Bibliothek verwenden.

Mit `urllib` können Sie eine Web-Seite ähnlich wie eine Datei behandeln. Sie geben einfach an, welche Webseite Sie abrufen möchten, und `urllib` kümmert sich um alle Details des HTTP-Protokolls und der Header.

Der äquivalente Code zum Lesen der Datei `romeo.txt` aus dem Web mit `urllib` sieht wie folgt aus:

\VerbatimInput{../code3/urllib1.py}

Sobald die Webseite mit `urllib.urlopen` geöffnet wurde, können wir sie wie eine Datei behandeln und mit einer `for`-Schleife durchlaufen.

Wenn das Programm läuft, sehen wir nur die Ausgabe des Inhalts der Datei. Die Kopfzeilen werden immer noch gesendet, aber der `urllib`-Code entfernt die Kopfzeilen und gibt nur die Daten an uns zurück.

~~~~
But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

Als Beispiel können wir ein Programm schreiben, um die Daten für `romeo.txt` abzurufen und die Häufigkeit jedes Worts in der Datei wie folgt zu berechnen:

\VerbatimInput{../code3/urlwords.py}

Auch hier können wir, nachdem wir die Webseite geöffnet haben, diese wie eine lokale Datei lesen.

Lesen von Binärdateien mit `urllib`
-----------------------------------

Manchmal möchten Sie eine Nicht-Text- (oder Binär-) Datei abrufen, wie z. B. eine Bild- oder Videodatei. Die Daten in diesen Dateien sind in der Regel nicht zum Ausgeben geeignet, aber wir können mit `urllib` leicht eine Kopie einer URL in eine lokale Datei auf einer Festplatte erstellen.

\index{Binärdatei}\index{binary}

Das Vorgehen besteht darin, die URL zu öffnen und mit `read` den gesamten Inhalt des Dokuments in eine String-Variable (`img`) herunterzuladen und diese Informationen dann wie folgt in eine lokale Datei zu schreiben:

\VerbatimInput{../code3/curl1.py}

Dieses Programm liest alle Daten auf einmal über das Netzwerk ein und speichert sie in der Variablen `img` im Hauptspeicher des Computers, öffnet dann die Datei `cover.jpg` und schreibt die Daten auf die Festplatte. Das Argument `wb` für `open()` öffnet eine Binärdatei nur zum Schreiben. Dieses Programm funktioniert, wenn die Größe der Datei kleiner ist als die Größe des Speichers Ihres Computers.

Wenn es sich jedoch um eine große Audio- oder Videodatei handelt, kann dieses Programm abstürzen oder zumindest extrem langsam laufen, wenn der Speicher Ihres Computers erschöpft ist. Um ein Überschreiten der Speicherkapazität zu vermeiden, werden die Daten in Blöcken (oder Puffern) abgerufen und dann jeder Block auf die Festplatte geschrieben, bevor der nächste Block abgerufen wird. Auf diese Weise kann das Programm Dateien beliebiger Größe lesen, ohne den gesamten Speicher des Computers zu verbrauchen.

\VerbatimInput{../code3/curl2.py}

In diesem Beispiel werden jeweils nur 100.000 Zeichen gelesen und dann in die Datei `cover.jpg` geschrieben, bevor die nächsten 100.000 Zeichen an Daten aus dem Web abgerufen werden.

Dieses Programm läuft wie folgt ab:

~~~~
python curl2.py
230210 characters copied.
~~~~

Parsen von HTML und Erkunden des Webs
-------------------------------------

\index{Web!Scraping}
\index{HTML parsen}

Eine der häufigsten Verwendungen der `urllib`-Fähigkeit in Python ist das *Scraping* des Webs. Beim Web-Scraping schreiben wir ein Programm, das vorgibt, ein Web-Browser zu sein, Seiten abruft, und dann die Daten auf diesen Seiten auf Muster untersucht.

Ein Beispiel: Eine Suchmaschine wie Google schaut sich die Quelle einer Webseite an, extrahiert die Links zu anderen Seiten und ruft diese Seiten auf, indem sie die Links extrahiert. Dies wird dann einfach wiederholt. Mit dieser Technik erkundet Google seinen Weg durch fast alle Seiten im Web.

Google verwendet auch die Häufigkeit der Links von Seiten, die es zu einer bestimmten Seite findet, als ein Maß dafür, wie „wichtig“ eine Seite ist und wie hoch die Seite in seinen Suchergebnissen erscheinen sollte.

Parsen von HTML mit regulären Ausdrücken
----------------------------------------

Eine einfache Möglichkeit, HTML zu parsen, ist die Verwendung regulärer Ausdrücke, um wiederholt nach Teilzeichenketten zu suchen und die zu extrahieren, die einem bestimmten Muster entsprechen.

Hier ist eine einfache Web-Seite:

~~~~ {.html}
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
~~~~

Wir können einen wohlgeformten regulären Ausdruck konstruieren, um die Links aus dem obigen Text wie folgt zu extrahieren:

~~~~
href="http[s]?://.+?"
~~~~

Unser regulärer Ausdruck sucht nach Zeichenfolgen, die mit `href=\"http://` oder `href=\"https://` beginnen, gefolgt von einem oder mehreren Zeichen (`.+?`) und einem weiteren doppelten Anführungszeichen. Das Fragezeichen hinter dem `[s]?` zeigt an, dass nach der Zeichenkette `http`, nach keinem oder einem "s", gesucht werden soll.

Das Fragezeichen hinter dem `.+?` zeigt an, dass der Abgleich im non-greedy-Modus und nicht "greedy" durchgeführt werden soll. Bei einem non-greedy-Match wird versucht, die *kleinstmögliche* übereinstimmende Zeichenfolge zu finden, bei einem greedy-Match wird versucht, die *größtmögliche* übereinstimmende Zeichenfolge zu finden.

\index{greedy}
\index{non-greedy}

Wir fügen unserem regulären Ausdruck Klammern hinzu, um anzugeben, welchen Teil der übereinstimmenden Zeichenfolge wir extrahieren möchten, und erzeugen das folgende Programm:

\index{Regex!runde Klammern}
\index{runde Klammern!regulärer Ausdruck}

\VerbatimInput{../code3/urlregex.py}

Die `ssl`-Bibliothek ermöglicht diesem Programm den Zugriff auf Websites, die HTTPS strikt erzwingen. Die Methode `read` gibt den HTML-Quellcode als Byte-Objekt zurück, anstatt ein HTTP-Response-Objekt zu liefern. Die Methode `findall` für reguläre Ausdrücke liefert uns eine Liste aller Zeichenketten, die mit unserem regulären Ausdruck übereinstimmen, und gibt nur den Linktext zwischen den Anführungszeichen zurück.

Wenn wir das Programm ausführen und eine URL eingeben, erhalten wir die folgende Ausgabe:

~~~~
Enter - https://docs.python.org
https://docs.python.org/3/index.html
https://www.python.org/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://www.python.org/
https://www.python.org/psf/donations/
http://sphinx.pocoo.org/
~~~~

Reguläre Ausdrücke funktionieren sehr gut, wenn ihr HTML gut formatiert und vorhersehbar ist. Da es aber eine Menge „defekter“ HTML-Seiten gibt, könnte eine Lösung, die nur reguläre Ausdrücke verwendet, entweder einige gültige Links übersehen oder mit unbrauchbaren Daten enden.

Dies kann durch die Verwendung einer robusten HTML-Parsing-Bibliothek gelöst werden.

Parsen von HTML mit BeautifulSoup
---------------------------------

\index{BeautifulSoup}

Auch wenn HTML wie XML aussieht^[Das XML-Format wird im nächsten Kapitel beschrieben.] und einige Seiten sorgfältig so konstruiert sind, dass sie XML sind, ist das meiste HTML in der Regel so fehlerhaft, dass ein XML-Parser die gesamte HTML-Seite als nicht korrekt formatiert zurückweist.

Es gibt eine Reihe von Python-Bibliotheken, die helfen können, HTML zu parsen und Daten aus den Seiten zu extrahieren. Jede der Bibliotheken hat ihre Stärken und Schwächen, und wir können eine nach unseren Bedürfnissen auswählen.

Als Beispiel werden wir einfach einige HTML-Eingaben parsen und Links mit Hilfe der *BeautifulSoup*-Bibliothek extrahieren. BeautifulSoup toleriert hochgradig fehlerhaftes HTML und lässt uns trotzdem einfach die benötigten Daten extrahieren. Auf der folgenden Seite können wir den BeautifulSoup-Code herunterladen und installieren:

<https://pypi.python.org/pypi/beautifulsoup4>

Informationen zur Installation von BeautifulSoup mit dem Python Package Index Tool `pip` finden sich unter:

<https://packaging.python.org/tutorials/installing-packages/>

Wir werden `urllib` benutzen, um die Seite zu lesen und dann `BeautifulSoup` benutzen, um die `href`-Attribute aus den Anker-Tags (`a`) zu extrahieren.

\index{BeautifulSoup}
\index{HTML}
\index{Parsen!HTML}

\VerbatimInput{../code3/urllinks.py}

Das Programm fragt nach einer Webadresse, öffnet dann die Webseite, liest die Daten ein und übergibt die Daten an den BeautifulSoup-Parser, der dann alle Anker-Tags abruft und das `href`-Attribut für jedes Tag ausgibt.

Wenn das Programm läuft, erzeugt es die folgende Ausgabe:

~~~~
Enter - https://docs.python.org
genindex.html
py-modindex.html
https://www.python.org/
#
whatsnew/3.6.html
whatsnew/index.html
tutorial/index.html
library/index.html
reference/index.html
using/index.html
howto/index.html
installing/index.html
distributing/index.html
extending/index.html
c-api/index.html
faq/index.html
py-modindex.html
genindex.html
glossary.html
search.html
contents.html
bugs.html
about.html
license.html
copyright.html
download.html
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
genindex.html
py-modindex.html
https://www.python.org/
#
copyright.html
https://www.python.org/psf/donations/
bugs.html
http://sphinx.pocoo.org/
~~~~

Diese Liste ist viel länger, weil einige HTML-Anker-Tags relative Pfade (z. B. `tutorial/index.html`) oder seiteninterne Verweise (z. B. `#`) sind, die nicht `http://` oder `https://` enthalten, was eine Voraussetzung in unserem regulären Ausdruck war.

Wir können auch BeautifulSoup verwenden, um verschiedene Teile der einzelnen Tags zu extrahieren:

\VerbatimInput{../code3/urllink2.py}

~~~~
python urllink2.py
Enter - http://www.dr-chuck.com/page1.htm
TAG: <a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>
URL: http://www.dr-chuck.com/page2.htm
Content: ['\nSecond Page']
Attrs: [('href', 'http://www.dr-chuck.com/page2.htm')]
~~~~

`html.parser` ist der HTML-Parser, der in der Standardbibliothek von Python 3 enthalten ist. Informationen zu anderen HTML-Parsern finden sich unter:

<http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser>

Diese Beispiele zeigen nur ansatzweise die Möglichkeiten von BeautifulSoup, wenn es um das Parsen von HTML geht.

Bonuskapitel für Unix-/Linux-User
---------------------------------

Wenn wir einen Linux-, Unix- oder Macintosh-Computer haben, haben wir wahrscheinlich Befehle in das Betriebssystem eingebaut, die sowohl Klartext- als auch Binärdateien über die Protokolle HTTP oder File Transfer (FTP) abrufen. Einer dieser Befehle ist `curl`:

\index{curl}

~~~~ {.bash}
$ curl -O http://www.py4e.com/cover.jpg
~~~~

Der Befehl `curl` ist die Abkürzung für „copy URL“ und so heißen die beiden zuvor aufgeführten Beispiele zum Abrufen von Binärdateien mit `urllib` auf [www.py4e.com/code3](http://www.py4e.com/code3) clevererweise `curl1.py` und `curl2.py`, da sie eine ähnliche Funktionalität wie der Befehl `curl` implementieren. Es gibt auch ein Beispielprogramm `curl3.py`, das diese Aufgabe etwas effektiver erledigt, für den Fall, dass der Leser dieses Vorgehen tatsächlich in einem Programm verwenden will.

Ein zweiter Befehl, der sehr ähnlich funktioniert, ist `wget`:

\index{wget}

~~~~ {.bash}
$ wget http://www.py4e.com/cover.jpg
~~~~

Diese beiden Befehle machen das Abrufen von Webseiten und entfernten Dateien zu einer einfachen Aufgabe.

Glossar
-------

BeautifulSoup
:   Eine Python-Bibliothek zum Parsen von HTML-Dokumenten und zum Extrahieren von Daten aus HTML-Dokumenten, die die meisten Unzulänglichkeiten im HTML kompensiert, die von Browsern im Allgemeinen ignoriert werden. Der BeautifulSoup-Code kann auf der folgenden Seite heruntergeladen werden: [www.crummy.com](http://www.crummy.com).
\index{BeautifulSoup}

Port
:   Eine Nummer, die im Allgemeinen angibt, mit welcher Anwendung wir Kontakt aufnehmen, wenn wir eine Socket-Verbindung zu einem Server herstellen. Ein Beispiel: Der Webverkehr verwendet normalerweise Port 80, während der E-Mail-Verkehr Port 25 verwendet.
\index{Port}

Scraping
:   Wenn ein Programm vorgibt, ein Webbrowser zu sein, und eine Webseite abruft, dann schaut es sich den Inhalt der Webseite an. Oft folgen Programme den Links auf einer Seite, um die nächste Seite zu finden, so dass sie ein Netzwerk von Seiten oder ein soziales Netzwerk durchqueren können.
\index{Socket}

Socket
:   Eine Netzwerkverbindung zwischen zwei Anwendungen, bei der die Anwendungen Daten in beide Richtungen senden und empfangen können.
\index{Socket}

Spider
:   Der Vorgang, bei dem eine Web-Suchmaschine eine Seite abruft und dann alle verlinkten Seiten auf dieser ebenfalls aufruft, bis sie fast alle Seiten im Internet besucht hat. Diese können dann zum Aufbau eines Suchindexes verwendet werden.
\index{Spider}

Übungen
-------

**Übung 1: Das Socket-Programm `socket1.py` soll so geändert werden, dass es den Benutzer nach der URL fragt und jede Webseite lesen kann. Hierbei kann `split('/')` verwendet werden, um die URL in ihre Bestandteile zu zerlegen, damit wir den Hostnamen für den Socket-Aufruf `connect` extrahieren können. Dabei soll eine Fehlerprüfung mit `try` und `except` hinzugefügt werden, um den Fall zu behandeln, dass der Benutzer eine falsch formatierte oder nicht existierende URL eingibt.**

**Übung 2: Das Socket-Programm soll so geändert werden, dass es die Anzahl der empfangenen Zeichen zählt und keinen Text mehr anzeigt, wenn es 3000 Zeichen angezeigt hat. Das Programm sollte das gesamte Dokument abrufen und die Gesamtzahl der Zeichen zählen und die Zählung der Anzahl der Zeichen am Ende des Dokuments anzeigen.**

**Übung 3: Es soll `urllib` verwendet werden, um die vorherige Übung zu wiederholen: (1) Abrufen des Dokuments von einer URL, (2) Anzeigen von bis zu 3000 Zeichen und (3) Zählen der Gesamtanzahl der Zeichen im Dokument. Dabei soll sich bei dieser Übung nicht um die Überschriften gekümmert werden, sondern lediglich die ersten 3000 Zeichen des Dokumentinhalts angezeigt werden.**

**Übung 4: Das Programm `urllinks.py` soll so geändert werden, dass es Paragraph-Tags (p) aus dem abgerufenen HTML-Dokument extrahiert und zählt sowie die Anzahl der Absätze als Ausgabe des Programms anzeigt. Dabei soll der Text dieser nicht angezeigt, sondern lediglich die Absätze gezählt werden. Das Programm soll auf mehreren kleinen sowie einigen größeren Webseiten getestet werden.**

**Übung 5: (Erweiterung) Das Socket-Programm soll so geändert werden, dass es nur Daten anzeigt, nachdem die Kopfzeilen und eine Leerzeile empfangen wurden. Es sollte bedacht werden, dass `recv` Zeichen (Zeilenumbrüche usw.) empfängt, nicht Zeilen.**

